{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/compas/test.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0829 12:51:28.394205 140477257193280 saver.py:1399] Restoring parameters from ../models/compas/test.model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from itertools import product, combinations\n",
    "import tensorflow.compat.v1 as tf \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from tensorflow.python.platform import flags\n",
    "from adf_data.census import census_data\n",
    "from adf_data.credit import credit_data\n",
    "from adf_data.compas import compas_data\n",
    "from adf_data.default import default_data\n",
    "from adf_data.bank import bank_data\n",
    "from adf_data.heart import heart_data\n",
    "from adf_model.tutorial_models import dnn\n",
    "from adf_utils.utils_tf import model_prediction, model_argmax , layer_out, model_eval\n",
    "from adf_utils.config import census, credit, bank, compas, default, heart\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "       \n",
    "def m_instance( sample, sens_params, conf):\n",
    "    index = []\n",
    "    m_sample = []\n",
    "    for sens in sens_params:\n",
    "        index.append([i for i in range(conf.input_bounds[sens - 1][0], conf.input_bounds[sens-1][1] + 1)])\n",
    "      \n",
    "    for ind in list(product(*index)):     \n",
    "        temp = sample.copy()\n",
    "        for i in range(len(sens_params)):\n",
    "            temp[0][sens_params[i]-1] = ind[i]\n",
    "        m_sample.append(temp)\n",
    "    return np.array(m_sample)\n",
    "    \n",
    "def clustering(probs,m_sample, sens_params):\n",
    "    epsillon = 0.025\n",
    "    cluster_dic = {}\n",
    "    cluster_dic['Seed'] = m_sample[0][0]\n",
    "         \n",
    "    for i in range(len(probs)):\n",
    "        #  to avoid k = Max + 1\n",
    "        if probs[i] == 1.0:\n",
    "            if (int( probs[i] / epsillon ) -1) not in cluster_dic.keys():\n",
    "             \n",
    "                cluster_dic[ (int( probs[i] / epsillon ) - 1)] = [ [m_sample[i][0][j - 1] for j in sens_params] ]\n",
    "           \n",
    "            else:\n",
    "                cluster_dic[ (int( probs[i] / epsillon ) - 1)].append( [m_sample[i][0][j - 1] for j in sens_params] )\n",
    "\n",
    "                       \n",
    "        elif int( probs[i] / epsillon ) not in cluster_dic.keys():\n",
    "                cluster_dic[ int( probs[i] / epsillon )] = [ [m_sample[i][0][j - 1] for j in sens_params] ]\n",
    "           \n",
    "        else:\n",
    "                cluster_dic[ int( probs[i] / epsillon)].append( [m_sample[i][0][j - 1] for j in sens_params] )\n",
    "\n",
    "    return cluster_dic  \n",
    "\n",
    "    \n",
    "def pred_prob(sess, x, preds, m_sample, input_shape):\n",
    "        probs = model_prediction(sess, x, preds, np.array(m_sample).reshape(len(m_sample),\n",
    "                                    input_shape[1]))[:,1:2].reshape(len(m_sample))\n",
    "        return probs        \n",
    "        \n",
    "def neuron_locator(sess, model, samples, layer_number,model_path, input_shape, \n",
    "                   nb_classes, dataset, sens_params, update_list ):\n",
    "        \n",
    "        if  sess._closed:\n",
    "#             config = tf.ConfigProto()\n",
    "#             config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "            config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "            config.allow_soft_placement= True            \n",
    "            sess   = tf.Session(config = config)\n",
    "            x      = tf.placeholder(tf.float32, shape = input_shape)\n",
    "            y      = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "            model  = dnn(input_shape, nb_classes)   \n",
    "            preds  = model(x)\n",
    "            saver  = tf.train.Saver()\n",
    "            saver.restore(sess, model_path)\n",
    "            \n",
    "        num_layers = len(model.layers)\n",
    "        feed_dic = {}\n",
    "        for neuron in range(len(update_list)):           \n",
    "            for layer in range(0,num_layers - 1,2):\n",
    "                if layer == 0:\n",
    "                    l = model.layers[layer].fprop(samples.astype('float32'))\n",
    "                else:\n",
    "                    l = model.layers[layer].fprop(r)                   \n",
    "                if layer + 1 == (layer_number * 2) - 1:\n",
    "                    indices = []\n",
    "                    for instance in range(l.shape[0]):                       \n",
    "                        indices.append([ instance, 0, neuron])       \n",
    "                    updates = [ update_list[ neuron ] ] * l.shape[0]\n",
    "                    r = model.layers[layer + 1].fprop(l , indices, updates)\n",
    "                else:\n",
    "                    r = model.layers[layer + 1].fprop(l)\n",
    "            feed_dic[neuron] = r\n",
    "        all_probs = sess.run(feed_dic)\n",
    "        out_dic   = {}\n",
    "        for key in all_probs.keys():\n",
    "            probs = np.array(all_probs[key]).reshape((9,2))[:,1:].reshape((9))\n",
    "            clus  = clustering(probs,samples, sens_params)\n",
    "            out_dic[key] = [len(clus) - 1 ]\n",
    "        sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        return out_dic\n",
    "    \n",
    "def model_acc(sess, model,model_path,input_shape, nb_classes,\n",
    "              dataset, sens_params,neuron,X,Y,layer_number,num_layers,update_list):\n",
    "        \n",
    "        if  sess._closed:\n",
    "#                 config = tf.ConfigProto()\n",
    "#                 config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "                config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "                config.allow_soft_placement= True\n",
    "                sess   = tf.Session(config = config)\n",
    "                x      = tf.placeholder(tf.float32, shape = input_shape)\n",
    "                y      = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "                model  = dnn(input_shape, nb_classes)   \n",
    "                preds  = model(x)\n",
    "                saver  = tf.train.Saver()\n",
    "                saver.restore(sess, model_path)\n",
    "        feed_dic = {}        \n",
    "        for layer in range(0,num_layers - 1,2):\n",
    "            if layer == 0:\n",
    "                l = model.layers[layer].fprop(X.astype('float32'))\n",
    "            else:\n",
    "                l = model.layers[layer].fprop(r)          \n",
    "            if layer + 1 == (layer_number * 2) - 1:\n",
    "                indices = []\n",
    "                for instance in range(l.shape[0]):                       \n",
    "                    indices.append([ instance, neuron])                \n",
    "                updates = [ update_list[ neuron ] ] * l.shape[0]                \n",
    "                r = model.layers[layer + 1].fprop(l , indices, updates)\n",
    "            else:\n",
    "                r = model.layers[layer + 1].fprop(l)             \n",
    "        all_probs = sess.run(r)\n",
    "        out_class = []\n",
    "        for out in all_probs:\n",
    "            out_class.append(np.argmax(out))\n",
    "        truth_val = []\n",
    "        for tr in Y:\n",
    "                truth_val.append(np.argmax(tr))\n",
    "        acc = 0\n",
    "        for i in range(len(out_class)):\n",
    "            if out_class[i] == truth_val[i]:\n",
    "                acc += 1\n",
    "        accuracy = round(acc/len(out_class),3)\n",
    "        sess.close()\n",
    "        tf.reset_default_graph()\n",
    "        return accuracy \n",
    "\n",
    "def get_rate(sess, model, model_path, input_shape, nb_classes,\n",
    "              dataset, lay_name, layer_output):\n",
    "        \n",
    "        def get_distance(vec1, vec2, size):\n",
    "            return abs(vec1 - vec2).sum() / size\n",
    "        \n",
    "        max_dis = 0\n",
    "        epsillon = 10 ** -7\n",
    "        num_samples = len(layer_output[lay_name])\n",
    "        #print('lay_name',lay_name)\n",
    "        layer_ind = np.where(np.array(list(layer_output.keys())) == lay_name)[0][0]\n",
    "\n",
    "        for ind in range(layer_ind):\n",
    "            temp_dis = 0\n",
    "            if 'ReLU' in np.array(list(layer_output.keys()))[ind]:\n",
    "                layer_name = np.array(list(layer_output.keys()))[ind]\n",
    "                layer_size  = len(layer_output[layer_name][0][0])\n",
    "                distances = np.zeros((num_samples,num_samples))\n",
    "                \n",
    "                for i in combinations(range(num_samples),2):\n",
    "                    distances[i[0],i[1]] = get_distance(layer_output[layer_name][i[0]],\n",
    "                                                        layer_output[layer_name][i[1]],layer_size)\n",
    "                if distances.max()> max_dis:\n",
    "                    max_dis = distances.max()                                      \n",
    "        distances = np.zeros((num_samples,num_samples))       \n",
    "        for i in combinations(range(num_samples),2):\n",
    "            distances[i[0],i[1]] = get_distance(layer_output[lay_name][i[0]],layer_output[lay_name][i[1]],len(layer_output[lay_name][0][0]))\n",
    "        cur_dis = distances.max()\n",
    "        change_rate = (cur_dis - max_dis ) / (max_dis + epsillon)\n",
    "        return change_rate\n",
    "    \n",
    "def layer_locator(sess, model, model_path,sens_params, input_shape, nb_classes,\n",
    "              dataset,conf, samples):\n",
    "        if  sess._closed:\n",
    "    #                 config = tf.ConfigProto()\n",
    "    #                 config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "            config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "            config.allow_soft_placement= True\n",
    "            sess   = tf.Session(config = config)\n",
    "            x      = tf.placeholder(tf.float32, shape = input_shape)\n",
    "            y      = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "            model  = dnn(input_shape, nb_classes)   \n",
    "            preds  = model(x)\n",
    "            saver  = tf.train.Saver()\n",
    "            saver.restore(sess, model_path)\n",
    "            \n",
    "        layer_list = []\n",
    "        for sample in samples:            \n",
    "            samples = m_instance( np.array([sample]) , sens_params, conf)\n",
    "            layer_output = layer_out(sess,model,np.array(samples).astype('float32')) \n",
    "            temp_list = []\n",
    "            for layer in layer_output.keys():\n",
    "                if 'ReLU' in layer:\n",
    "                    temp_rate = get_rate(sess, model, model_path, input_shape, nb_classes,\n",
    "                                          dataset,layer, layer_output)\n",
    "                    temp_list.append(temp_rate)             \n",
    "            layer_list.append((np.argmax(np.array(temp_list[1:])) + 2 ))\n",
    "        sess.close()\n",
    "        tf.reset_default_graph()    \n",
    "        return stats.mode(layer_list)[0][0]      \n",
    "#-------------------------------------------\n",
    "    \n",
    "def dnn_fair_testing(dataset, sens_params, model_path):\n",
    "\n",
    "    data = {\"census\":census_data, \"credit\":credit_data, \"bank\":bank_data, \"compas\":compas_data, \n",
    "            \"default\": default_data, \"heart\":heart_data}\n",
    "    data_config = {\"census\":census, \"credit\":credit, \"bank\":bank, \"compas\":compas, \"default\":default,\n",
    "                  \"heart\":heart}\n",
    "    \n",
    "    # prepare the testing data and model\n",
    "    X, Y, input_shape, nb_classes = data[dataset]()\n",
    "    tf.set_random_seed(1234)\n",
    "\n",
    "#     config = tf.ConfigProto()\n",
    "#     config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "    \n",
    "#     config = tf.ConfigProto(intra_op_parallelism_threads=16, \n",
    "#                             inter_op_parallelism_threads=2, allow_soft_placement=True, device_count = {'CPU': 16})\n",
    "    for trial in range(9,10,1):\n",
    "        config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "        config.allow_soft_placement= True\n",
    "\n",
    "       # with tf.device('/CPU:1'):\n",
    "        sess  = tf.Session(config = config)\n",
    "        x     = tf.placeholder(tf.float32, shape = input_shape)\n",
    "        y     = tf.placeholder(tf.float32, shape = (None, nb_classes))\n",
    "        model = dnn(input_shape, nb_classes)   \n",
    "\n",
    "        preds = model(x)\n",
    "        saver = tf.train.Saver()\n",
    "        model_path ='../models/'\n",
    "        model_path = model_path + dataset + \"/test.model\"\n",
    "        \n",
    "        saver.restore(sess, model_path)\n",
    "        eval_params = {'batch_size': 128}\n",
    "        ini_acc = round(model_eval(sess, x, y, preds, X, Y, args=eval_params),3)\n",
    "        num_trial = 11\n",
    "        num_rand_point_1 = 4 # number of random points in range(0 to mean)\n",
    "        num_rand_point_2 = 3 # number of random points in range(mean to std)\n",
    "        num_rand_point_3 = 1 # number of random points in range(mean + std to  mean + 2 * std)\n",
    "\n",
    "    # Loading the result of QID\n",
    "        layer_output = layer_out(sess,model,X.astype('float32'))\n",
    "        input_df  = pd.read_csv('../results/' + dataset + '/OurTool/RQ3/total_disc_'+str(trial)+'.csv',header='infer')\n",
    "        input_df = input_df.drop(columns=['Unnamed: 0'])\n",
    "        sample_df = input_df.copy()\n",
    "        sample_df_rand = sample_df.sample(n = 900,axis = 0,random_state = np.random.RandomState())\n",
    "        sample_df_maxk = sample_df.sort_values(by = 'k',ascending=False).head(100)\n",
    "        sample_df = pd.concat([sample_df_rand,sample_df_maxk])\n",
    "        ini_k_samples = sample_df['k']\n",
    "        sample_df = sample_df.drop(columns = ['min','k']) \n",
    "        samples   = sample_df.to_numpy()\n",
    "        num_samples = len(samples)\n",
    "        print(num_samples)\n",
    "        #np.save('../results/'+dataset+'/RQ3/samples_'+str(num_samples)+'_'+str(trial)+'.npy', samples)\n",
    "        #update_df = pd.read_csv('../results/'+dataset+'/dataset_layer2_out.csv')\n",
    "        layer_number = layer_locator(sess, model, model_path, sens_params, input_shape, nb_classes,\n",
    "                  dataset,data_config[dataset], samples)\n",
    "        #layer_number = 2\n",
    "        print('Biased Layer',layer_number)\n",
    "        #-----------------------------\n",
    "        update_df = layer_output['ReLU'+str((2*layer_number) -1 )]\n",
    "        update_min  = np.min(update_df,axis=0)\n",
    "        update_max  = np.max(update_df,axis=0)\n",
    "        update_mean = np.mean(update_df,axis=0)\n",
    "        update_std  = np.std(update_df,axis=0)\n",
    "        update_list = []\n",
    "        update_list.append(update_min)\n",
    "        rand_point_1 = np.sort(np.random.random(num_rand_point_1))\n",
    "        for i in range(len(rand_point_1)):\n",
    "            update_list.append(rand_point_1[i] * update_mean)\n",
    "        update_list.append(update_mean)\n",
    "        rand_point_2 = np.sort(np.random.random(num_rand_point_2))\n",
    "        for i in range(len(rand_point_2)):\n",
    "            update_list.append(update_mean + rand_point_2[i] * update_std)\n",
    "        update_list.append(update_mean + update_std +  np.random.random(num_rand_point_3)[0] * update_std)        \n",
    "        update_list.append(update_max)\n",
    "\n",
    "        #----------------------------------\n",
    "    #     layer_number = layer_locator(sess, model, model_path, sens_params, input_shape, nb_classes,\n",
    "    #           dataset,data_config[dataset], samples) #supposed to come from layer locator\n",
    "    #     print(layer_number)\n",
    "        layer_size   = model.layers[(layer_number*2) - 1].input_shape[1]\n",
    "        layer_name   = model.layers[(layer_number*2) - 1]\n",
    "        num_layers   = len(model.layers)\n",
    "\n",
    "        all_dic = {}\n",
    "        accu_neuron = {}\n",
    "        acc_try = {}\n",
    "        sample_ind = 0\n",
    "        for sample in samples:       \n",
    "            update_list_man = np.array([0] * layer_size)\n",
    "            m_samples  = m_instance( np.array([sample]), sens_params, data_config[dataset])\n",
    "            change_dic = {}\n",
    "            for i in range(num_trial):\n",
    "                update_list_man = update_list[i]               \n",
    "                x = neuron_locator(sess, model, m_samples, layer_number,model_path,\n",
    "                               input_shape, nb_classes, dataset, sens_params, update_list_man )\n",
    "                if sample_ind == 0:\n",
    "                    accu_neuron = {}\n",
    "                    for neuron in range(len(update_list_man)):\n",
    "                        accu_neuron[neuron] = model_acc(sess, model,model_path,\n",
    "                                         input_shape, nb_classes, dataset, sens_params,\n",
    "                                         neuron,X,Y,layer_number,num_layers,update_list_man)\n",
    "                    acc_try[i] = accu_neuron                 \n",
    "                change_dic[i] = x  \n",
    "            all_dic[sample_ind] = change_dic\n",
    "            clear_output(wait=True)\n",
    "            sample_ind += 1\n",
    "\n",
    "        # create the folder for storing the fairness testing result\n",
    "        if not os.path.exists('../results/'):\n",
    "            os.makedirs('../results/')\n",
    "        if not os.path.exists('../results/' + dataset + '/'):\n",
    "            os.makedirs('../results/' + dataset + '/')\n",
    "        if not os.path.exists('../results/' + dataset + '/OurTool/'):\n",
    "            os.makedirs('../results/' + dataset + '/OurTool/')\n",
    "        if not os.path.exists('../results/' + dataset + '/OurTool/RQ3/'):\n",
    "            os.makedirs('../results/' + dataset + '/OurTool/RQ3/')          \n",
    "        if not os.path.exists('../results/' + dataset + '/OurTool/RQ3/table2/'):\n",
    "            os.makedirs('../results/' + dataset + '/OurTool/RQ3/table2/')\n",
    "        np.save('../results/'+dataset+'/OurTool/RQ3/table2/inik_'+str(num_samples)+'_'+str(trial)+'.npy',\n",
    "                np.array(ini_k_samples))\n",
    "        np.save('../results/'+dataset+'/OurTool/RQ3/table2/accu_dic_'+str(num_samples)+'_'+str(trial)+'.npy',\n",
    "                acc_try) \n",
    "        np.save('../results/'+dataset+'/OurTool/RQ3/table2/all_dic_'+str(num_samples)+'_'+str(trial)+'.npy', \n",
    "                all_dic)   \n",
    "\n",
    "        accu_dic = dict(np.load('../results/'+dataset+'/OurTool/RQ3/table2/accu_dic_'+str(num_samples)+'_'+str(trial)+'.npy',\n",
    "                                allow_pickle=True).item())  \n",
    "        all_dic  = dict(np.load('../results/'+dataset+'/OurTool/RQ3/table2/all_dic_'+str(num_samples)+'_'+str(trial)+'.npy',\n",
    "                               allow_pickle=True).item())\n",
    "        ini_k = np.load('../results/'+dataset+'/OurTool/RQ3/table2/inik_'+str(num_samples)+'_'+str(trial)+'.npy')\n",
    "\n",
    "        num_samples = len(all_dic.keys())\n",
    "        num_force   = len(all_dic[0].keys())\n",
    "        num_neuron  = len(all_dic[0][0].keys())\n",
    "        ini_k = np.repeat(ini_k, (num_force * num_neuron))\n",
    "        data  = np.zeros(((num_samples * num_force * num_neuron) ,4) , dtype = 'int32')\n",
    "        df    = pd.DataFrame(data,columns = ['sample','force','neuron','K'],dtype = 'int32')\n",
    "        sample_col = np.repeat(np.array([i for i in range(num_samples)]),(num_neuron * num_force))\n",
    "        force_col  = np.array([ int(i/num_neuron ) for i in range( num_neuron * num_force ) ] * num_samples)\n",
    "        neuron_col = np.array([i for i in range( num_neuron )] * ( num_samples*num_force ))\n",
    "        df['sample'] = sample_col\n",
    "        df['force']  = force_col\n",
    "        df['neuron'] = neuron_col\n",
    "        df['acc'] = 0\n",
    "        acc = pd.DataFrame(accu_dic).transpose().to_numpy()\n",
    "        acc = acc.reshape(acc.shape[0] * acc.shape[1],)\n",
    "        for i in range(len(all_dic.keys())):\n",
    "            temp = pd.DataFrame(all_dic[i]).transpose().to_numpy()\n",
    "            temp = temp.reshape(((len(all_dic[0][0].keys())) * len(all_dic[0].keys()),))   \n",
    "            df.loc[df.loc[(df['sample'] == i) ].index,'acc'] = acc\n",
    "            df.loc[df.loc[(df['sample'] == i) ].index,'K'] = temp\n",
    "        df['K'] = df['K'].transform(lambda x:x[0])\n",
    "        df['init_k'] = ini_k\n",
    "\n",
    "        R_act   = []\n",
    "        R_deact = []\n",
    "        diff_R  = []\n",
    "        acc_e   = 0.05\n",
    "        for neuron in range(num_neuron):\n",
    "            k_deact = df.loc[(df['neuron'] == neuron) & (df['force'] <= 1) & \\\n",
    "                                  (df['acc'] >= ini_acc - acc_e)]['K'].mean()\n",
    "            k_act   = df.loc[(df['neuron'] == neuron) & (df['force'] > 1) & \\\n",
    "                                  (df['acc'] >= ini_acc - acc_e)]['K'].mean()\n",
    "            k_init  = df.loc[(df['neuron'] == neuron) & (df['acc'] >= ini_acc - acc_e)]['init_k'].mean()\n",
    "            R_act_temp   = (k_act - k_init) / k_init\n",
    "            R_deact_temp = (k_deact - k_init) / k_init\n",
    "            diff_R_temp  = R_act_temp - R_deact_temp\n",
    "            R_act.append(R_act_temp)\n",
    "            R_deact.append(R_deact_temp)\n",
    "            diff_R.append(diff_R_temp)\n",
    "\n",
    "\n",
    "        df.to_csv('../results/'+dataset+'/OurTool/RQ3/table2/df_'+str(num_samples)+'_'+str(trial)+'.csv',index=False)\n",
    "        np.save('../results/'+dataset+'/OurTool/RQ3/table2/R_act_'+str(num_samples)+'_'+str(trial)+'.npy',R_act)\n",
    "        np.save('../results/'+dataset+'/OurTool/RQ3/table2/R_deact_'+str(num_samples)+'_'+str(trial)+'.npy',R_deact)\n",
    "        np.save('../results/'+dataset+'/OurTool/RQ3/table2/R_diffR_'+str(num_samples)+'_'+str(trial)+'.npy',diff_R)\n",
    "def main(argv = None):\n",
    "    time1 = time.time()\n",
    "    dnn_fair_testing(dataset = FLAGS.dataset, \n",
    "                     sens_params = FLAGS.sens_params,\n",
    "                     model_path  = FLAGS.model_path)\n",
    "    print(time.time() - time1 )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    flags.DEFINE_string(\"dataset\", \"compas\", \"the name of dataset\")\n",
    "    flags.DEFINE_string('model_path', '../models/', 'the path for testing model')\n",
    "    flags.DEFINE_list('sens_params',[3,2,1],'sensitive parameters index.1 for age, 9 for gender, 8 for race')\n",
    "    tf.app.run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = 'credit'\n",
    "num_samples = 1000\n",
    "for trial in range(5):\n",
    "    \n",
    "    df=pd.read_csv('../results/'+dataset+'/OurTool/RQ3/table2/df_'+str(num_samples)+'_'+str(trial)+'.csv')\n",
    "    R_act   = []\n",
    "    R_deact = []\n",
    "    diff_R  = []\n",
    "    ini_acc = 0.976\n",
    "    acc_e   = 0.05\n",
    "    for neuron in range(df['neuron'].max() +1):\n",
    "        k_deact = df.loc[(df['neuron'] == neuron) & (df['force'] <= 1) & \\\n",
    "                              (df['acc'] >= ini_acc - acc_e)]['K'].mean()\n",
    "        k_act   = df.loc[(df['neuron'] == neuron) & (df['force'] > 1) & \\\n",
    "                              (df['acc'] >= ini_acc - acc_e)]['K'].mean()\n",
    "        k_init  = df.loc[(df['neuron'] == neuron) & (df['acc'] >= ini_acc - acc_e)]['init_k'].mean()\n",
    "        R_act_temp   = (k_act - k_init) / k_init\n",
    "        R_deact_temp = (k_deact - k_init) / k_init\n",
    "        diff_R_temp  = R_act_temp - R_deact_temp\n",
    "        R_act.append(R_act_temp)\n",
    "        R_deact.append(R_deact_temp)\n",
    "        diff_R.append(diff_R_temp)\n",
    "\n",
    "\n",
    "    #df.to_csv('../results/'+dataset+'/OurTool/RQ3/table2/df_'+str(num_samples)+'_'+str(trial)+'.csv',index=False)\n",
    "    np.save('../results/'+dataset+'/OurTool/RQ3/table2/R_act_'+str(num_samples)+'_'+str(trial)+'.npy',R_act)\n",
    "    np.save('../results/'+dataset+'/OurTool/RQ3/table2/R_deact_'+str(num_samples)+'_'+str(trial)+'.npy',R_deact)\n",
    "    np.save('../results/'+dataset+'/OurTool/RQ3/table2/R_diffR_'+str(num_samples)+'_'+str(trial)+'.npy',diff_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = 'credit'\n",
    "num_samples = 1000\n",
    "trial = 0\n",
    "df=pd.read_csv('../results/'+dataset+'/OurTool/RQ3/table2/df_'+str(num_samples)+'_'+str(trial)+'.csv')\n",
    "R_act=np.load('../results/'+dataset+'/OurTool/RQ3/table2/R_act_'+str(num_samples)+'_'+str(trial)+'.npy')\n",
    "R_deact=np.load('../results/'+dataset+'/OurTool/RQ3/table2/R_deact_'+str(num_samples)+'_'+str(trial)+'.npy')\n",
    "diff_R=np.load('../results/'+dataset+'/OurTool/RQ3/table2/R_diffR_'+str(num_samples)+'_'+str(trial)+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 18 17\n",
      "0.029610778443113772 -0.025731394354148757 0.0\n",
      "5 10 9\n",
      "-0.03658682634730545 -0.003173652694610729 0.01555175363558603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1_pos = np.argmax(diff_R)\n",
    "cae_n1 = diff_R[n1_pos]\n",
    "n2_pos = np.argmax(np.delete(diff_R, n1_pos))\n",
    "cae_n2 = diff_R[n2_pos]\n",
    "n3_pos = np.argmax(np.delete(diff_R, [n1_pos,n2_pos]))\n",
    "cae_n3 = diff_R[n3_pos]\n",
    "print(n1_pos,n2_pos,n3_pos)\n",
    "print(cae_n1,cae_n2,cae_n3)\n",
    "n1_pos = np.argmin(diff_R)\n",
    "cae_n1 = diff_R[n1_pos]\n",
    "n2_pos = np.argmin(np.delete(diff_R, n1_pos))\n",
    "cae_n2 = diff_R[n2_pos]\n",
    "n3_pos = np.argmin(np.delete(diff_R, [n1_pos,n2_pos]))\n",
    "cae_n3 = diff_R[n3_pos]\n",
    "print(n1_pos,n2_pos,n3_pos)\n",
    "print(cae_n1,cae_n2,cae_n3)\n",
    "df['neuron'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>force</th>\n",
       "      <th>neuron</th>\n",
       "      <th>K</th>\n",
       "      <th>acc</th>\n",
       "      <th>init_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.828</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.805</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.731</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.655</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.631</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351841</th>\n",
       "      <td>999</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.631</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351873</th>\n",
       "      <td>999</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.629</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351905</th>\n",
       "      <td>999</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.627</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351937</th>\n",
       "      <td>999</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.593</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351969</th>\n",
       "      <td>999</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.606</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sample  force  neuron  K    acc  init_k\n",
       "65           0      2       1  4  0.828       5\n",
       "97           0      3       1  4  0.805       5\n",
       "129          0      4       1  3  0.731       5\n",
       "161          0      5       1  3  0.655       5\n",
       "193          0      6       1  3  0.631       5\n",
       "...        ...    ...     ... ..    ...     ...\n",
       "351841     999      6       1  2  0.631      11\n",
       "351873     999      7       1  2  0.629      11\n",
       "351905     999      8       1  2  0.627      11\n",
       "351937     999      9       1  3  0.593      11\n",
       "351969     999     10       1  1  0.606      11\n",
       "\n",
       "[9000 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[(df['neuron'] == 1) & (df['force'] > 1) & \\\n",
    "                              (df['acc'] >= ini_acc - acc_e)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.20256709,         nan, -0.14725788, -0.14181252, -0.13708951,\n",
       "       -0.27240373, -0.14749125, -0.26801504, -0.10110852, -0.0949825 ,\n",
       "       -0.06826138, -0.21149358, -0.0873979 , -0.07082847, -0.0949825 ,\n",
       "       -0.24384481, -0.23124271, -0.22721704, -0.0949825 , -0.09028171,\n",
       "       -0.16723454, -0.0949825 ,         nan, -0.13514845, -0.0722287 ,\n",
       "       -0.28941074, -0.09516401, -0.18231557, -0.28354726, -0.26272527,\n",
       "       -0.09795799,         nan])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_act\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10886814, -0.0655776 , -0.17724621, -0.09789965, -0.08168028,\n",
       "       -0.12170362, -0.08098016, -0.12823804, -0.11796966, -0.0949825 ,\n",
       "       -0.06347725, -0.09976663, -0.09311552, -0.07432905, -0.0949825 ,\n",
       "       -0.1529755 , -0.14667445, -0.09801634, -0.0949825 , -0.09206534,\n",
       "       -0.10198366, -0.0949825 , -0.11901984, -0.09124854, -0.08471412,\n",
       "       -0.08063011, -0.09614936, -0.12228705, -0.12917153, -0.16067678,\n",
       "       -0.09533256, -0.06861144])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_deact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09369895,         nan,  0.02998833, -0.04391287, -0.05540923,\n",
       "       -0.15070012, -0.06651109, -0.139777  ,  0.01686114,  0.        ,\n",
       "       -0.00478413, -0.11172695,  0.00571762,  0.00350058,  0.        ,\n",
       "       -0.09086931, -0.08456826, -0.1292007 ,  0.        ,  0.00178363,\n",
       "       -0.06525088,  0.        ,         nan, -0.04389991,  0.01248541,\n",
       "       -0.20878063,  0.00098535, -0.06002852, -0.15437573, -0.10204849,\n",
       "       -0.00262544,         nan])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
